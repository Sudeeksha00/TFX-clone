{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mental-magnitude",
   "metadata": {},
   "source": [
    "## To Google Cloud Platform\n",
    "\n",
    "As we collect more data, the data validation becomes a more time-consuming step in our machine learning workflow. One way of reducing the time to perform the validation is by taking advantage of available cloud solutions. By using a cloud provider, we aren’t limited to the computation power of our laptop or on-premise computing\n",
    "resources.\n",
    "\n",
    "As an example, we’ll introduce how to run TFDV on Google Cloud’s product Dataflow. TFDV runs on Apache Beam, which makes a switch to GCP Dataflow very easy.Dataflow lets us accelerate our data validation tasks by parallelizing and distributing them across the allocated nodes for our data-processing task. While Dataflow charges\n",
    "for the number of CPUs and the gigabytes of memory allocated, it can speed up our pipeline step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-revelation",
   "metadata": {},
   "source": [
    "**DataFlow**\n",
    "\n",
    "Dataflow is a managed service for executing a wide variety of data processing patterns. One can deploy batch and streaming data processing pipelines using Dataflow, including directions for using service features.\n",
    "\n",
    "We all know that Google probably has more experience processing big data than any other organization on the planet and now they’re making their data processing software available to their customers. Not only that, but they’ve also open-sourced the software as Apache Beam.\n",
    "\n",
    "Cloud Dataflow is a serverless data processing service that runs jobs written using the Apache Beam libraries. When you run a job on Cloud Dataflow, it spins up a cluster of virtual machines, distributes the tasks in your job to the VMs, and dynamically scales the cluster based on how the job is performing. It may even change the order of operations in your processing pipeline to optimize your job. You don't need to take care of creating and managing \n",
    "VM's for huge processing jobs.\n",
    "\n",
    "\n",
    "**Apache Beam**\n",
    "\n",
    "The Apache Beam SDK is an open source programming model that enables you to develop both batch and streaming pipelines. You create your pipelines with an Apache Beam program and then run them on the Dataflow service. The [Apache Beam documentation](#https://beam.apache.org/) provides in-depth conceptual information and reference material for the Apache Beam programming model, SDKs, and other runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions\n",
    "\n",
    "PROJECT_ID = '<YOUR_GCP_PROJECT_ID>'\n",
    "JOB_NAME = '<YOUR_JOB_NAME>'\n",
    "GCS_STAGING_LOCATION = 'gs://<YOUR_GCP_BUCKET>/staging'\n",
    "GCS_TMP_LOCATION = 'gs://<YOUR_GCP_BUCKET>/tmp'\n",
    "GCS_DATA_LOCATION = 'gs\"//<YOUR_GCP_BUCKET/<FILE_nAME>.tfrecord'\n",
    "GCS_STATS_OUTPUT_PATH = 'gs://<YOUR_GCP_BUCKET>/output'\n",
    "\n",
    "PATH_TO_WHL_FILE = '<PATH_TO_YOUT_WHEEL_FILE>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and set your PipelineOptions.\n",
    "options = PipelineOptions()\n",
    "\n",
    "# For Cloud execution, set the Cloud Platform project, job_name,\n",
    "# staging location, temp_location and specify DataflowRunner.\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = PROJECT_ID\n",
    "google_cloud_options.job_name = JOB_NAME\n",
    "google_cloud_options.staging_location = GCS_STAGING_LOCATION\n",
    "google_cloud_options.temp_location = GCS_TMP_LOCATION\n",
    "options.view_as(StandardOptions).runner = 'DataflowRunner'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-gates",
   "metadata": {},
   "source": [
    "Once we have configured the Google Cloud options, we need to configure the setup for the Dataflow workers. All tasks are executed on workers that need to be provisioned with the necessary packages to run their tasks. In our case, we need to install TFDV by specifying it as an additional package.\n",
    "\n",
    "To do this, download the latest TFDV package (the binary .whl file) 3 to your local system. Choose a version which can be executed on a Linux system (e.g., tensorflow_data_validation-0.22.0-cp37-cp37m-manylinux2010_x86_64.whl ).\n",
    "\n",
    "Data ValidationTo configure the worker setup options, specify the path to the downloaded package\n",
    "in the setup_options.extra_packages list as shown\n",
    "\n",
    "Find TFDV wheel file [here](https://pypi.org/project/tensorflow-data-validation/#files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_options = options.view_as(SetupOptions)\n",
    "setup_options.extra_packages = [PATH_TO_WHL_FILE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.generate_statistics_from_tfrecord(GCS_DATA_LOCATION,\n",
    "                                       output_path=GCS_STATS_OUTPUT_PATH,\n",
    "                                       pipeline_options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-riding",
   "metadata": {},
   "source": [
    "After you have started the data validation with Dataflow, you can switch back to the Google Cloud console. Your newly kicked off job should be listed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
