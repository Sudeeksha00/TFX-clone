{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.getcwd().split(os.sep)[:-3]\n",
    "root_dir = '/'.join(root_dir)\n",
    "sys.path.append(root_dir)\n",
    "from utils.helper_metastore import *\n",
    "from utils.configurations.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-domain",
   "metadata": {},
   "source": [
    "### Custom data connector\n",
    "\n",
    "To compute data statistics, TFDV provides several convenient methods for handling input data in various formats (e.g. TFRecord of tf.train.Example, CSV, DataFrame etc). If your data format is not in this list, you need to write a custom data connector for reading input data, and connect it with the TFDV core API for computing data statistics.\n",
    "\n",
    "The TFDV core API for computing data statistics is a Beam PTransform that takes a PCollection of batches of input examples (a batch of input examples is represented as an Arrow RecordBatch), and outputs a PCollection containing a single DatasetFeatureStatisticsList protocol buffer.\n",
    "\n",
    "Once you have implemented the custom data connector that batches your input examples in an Arrow RecordBatch, you need to connect it with the tfdv.GenerateStatistics API for computing the data statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "from tensorflow_data_validation.coders import tf_example_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-incident",
   "metadata": {},
   "source": [
    "Let consider the scenario were we want to generate statistics of the dataset from text file. so, we decided to write custom conectore which follows the flow as:\n",
    "\n",
    "   >Read_Text_File -> Serialize Row into tf.train.Example -> DecodeData -> GenerateStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions to tf.train.Example\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# Function which converts the row to tf.train.Example\n",
    "def serialize_example(row):\n",
    "    headers = ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
    "               'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "    integer_columns = ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch']\n",
    "    float_columns = ['Fare']\n",
    "    row = row.split('|')\n",
    "    feature = {}\n",
    "    for idx in range(len(headers)):\n",
    "        if headers[idx] in integer_columns:\n",
    "            value = -999 if row[idx] == '' else row[idx]\n",
    "            feature[headers[idx]] = _int64_feature(int(float(value)))\n",
    "        elif headers[idx] in float_columns:\n",
    "            value = -999.0 if row[idx] == '' else row[idx]\n",
    "            feature[headers[idx]] = _float_feature(float(value))\n",
    "        else:\n",
    "            value = 'None' if row[idx] == '' else row[idx]\n",
    "            feature[headers[idx]] = _bytes_feature(value.strip().encode())\n",
    "            \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "dataset_name = Config.ADD_ONS_DATASET_NAME + '.txt'\n",
    "INPUT_LOCATION = os.path.join(root_dir, 'data', dataset_name)\n",
    "OUTPUT_LOCATION = os.path.join(root_dir, 'outputs')\n",
    "\n",
    "if not os.path.exists(OUTPUT_LOCATION):\n",
    "    os.makedirs(OUTPUT_LOCATION)\n",
    "    \n",
    "OUTPUT_LOCATION = os.path.join(OUTPUT_LOCATION, 'statistics.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    stats = (\n",
    "    p | 'Readtxt' >> beam.io.ReadFromText(INPUT_LOCATION,\n",
    "                                          skip_header_lines = 1)\n",
    "      | 'Serialize to tf.Example' >> beam.Map(serialize_example)\n",
    "      | 'DecodeData' >> tf_example_decoder.DecodeTFExample()\n",
    "      | 'GenerateStatistics' >> tfdv.GenerateStatistics()\n",
    "    )\n",
    "    \n",
    "    _ = (stats |  'WriteStatsOutput' >> tfdv.WriteStatisticsToTFRecord(OUTPUT_LOCATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.load_statistics(OUTPUT_LOCATION)\n",
    "\n",
    "schema = tfdv.infer_schema(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-genius",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-trial",
   "metadata": {},
   "source": [
    "We can found some nan value(-999) in Age column. let consider we discus about this with our domain experts, they conformed that Age column is optional which will available 0.8% times in records.\n",
    "\n",
    "TFDV automatical infers such column as required, we have to tweak this manually in schema file. Let see how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(schema, 'Age').presence.min_fraction = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)\n",
    "\n",
    "# Now you can see that Age column is marked as optional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
