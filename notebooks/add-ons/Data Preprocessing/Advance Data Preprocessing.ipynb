{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.getcwd().split(os.sep)[:-3]\n",
    "root_dir = '/'.join(root_dir)\n",
    "sys.path.append(root_dir)\n",
    "from utils.helper_metastore import *\n",
    "from utils.configurations.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'absl')\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-mixture",
   "metadata": {},
   "source": [
    "## Advance TensorFlow Transform\n",
    "\n",
    "TensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset. TensorFlow has built-in support for manipulations on a single example or a batch of examples. tf.Transform extends these capabilities to support full passes over the entire training dataset.\n",
    "\n",
    "The output of tf.Transform is exported as a TensorFlow graph which we can use for both training and serving. Using the same graph for both training and serving can prevent skew, since the same transformations are applied in both stages.\n",
    "\n",
    "In this notebook, we are going to pick a scenario were we are going to read the data from the disk and define schema of the given dataset to do transformation using apache mean and tf transform.\n",
    "\n",
    "Follwing fingure shows the components of our transformation pipeline\n",
    "\n",
    "![fingure_3](../../image/data_preprocessing_fig_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "import apache_beam as beam\n",
    "from tensorflow_transform.tf_metadata.dataset_metadata import DatasetMetadata\n",
    "from tensorflow_transform.tf_metadata.schema_utils import schema_from_feature_spec\n",
    "\n",
    "from tfx_bsl.public import tfxio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-crossing",
   "metadata": {},
   "source": [
    "Here we are going to use the titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extention = '.csv'\n",
    "train_datafile = os.path.join(root_dir, Config.ADD_ONS_DATASET_PATH\n",
    "                             , Config.ADD_ONS_DATASET_NAME +extention)\n",
    "\n",
    "OUPUT_PATH = os.path.join(root_dir, Config.ADD_ONS_DATASET_PATH, \n",
    "                          Config.ADD_ONS_DATASET_NAME + '.tfrecord')\n",
    "raw_data = pd.read_csv(train_datafile, sep = '|')\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining all columns and clump columns based on their data type\n",
    "columns = list(raw_data.columns)\n",
    "INT_COLUMNS = ['PassengerId', 'Survived', 'Pclass', 'SibSp', 'Parch']\n",
    "FLOAT_COLUMNS = ['Age', 'Fare']\n",
    "STRING_COLUMNS = ['Name', 'Sex', 'Ticket', 'Embarked']\n",
    "\n",
    "# defining transform to the required columns\n",
    "ONE_HOT_FEATURES = {'Sex': 2, 'Embarked': 4}\n",
    "TEXT_FEATURES = {'Name' : None}\n",
    "FLOAT_FEATURE = {'Age': True, 'Fare': True}\n",
    "LABEL_KEY = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-morning",
   "metadata": {},
   "source": [
    "Let's define a schema based for the columns are in our input. Among other things this will help with importing them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating metadata for an dataset\n",
    "meta_data = DatasetMetadata(\n",
    "            schema_from_feature_spec(dict(\n",
    "            [\n",
    "                (name, tf.io.FixedLenFeature([], tf.string))\n",
    "                for name in STRING_COLUMNS\n",
    "            ] +\n",
    "            [\n",
    "                (name, tf.io.FixedLenFeature([], tf.int64))\n",
    "                for name in INT_COLUMNS\n",
    "            ] +\n",
    "            [\n",
    "                (name, tf.io.FixedLenFeature([], tf.float32))\n",
    "                for name in FLOAT_COLUMNS\n",
    "            ]+\n",
    "            [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n",
    "            ))\n",
    ")\n",
    "\n",
    "SCHEMA = meta_data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-clinton",
   "metadata": {},
   "source": [
    "Here we had defined some helper function and preprocessing function will is used to transform our raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-harvey",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "# Some of our features are of a sparse nature, but TFT expects the transformation outputs\n",
    "# to be dense\n",
    "# Following function will be used to convert the sparse into dense tensor\n",
    "def fill_in_missing(x):\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    if type(x) == tf.SparseTensor:\n",
    "        x = tf.sparse.to_dense(\n",
    "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "            default_value)\n",
    "    if len(x.shape) > 1:\n",
    "        x = tf.squeeze(x, axis = 1)\n",
    "    return x\n",
    "\n",
    "def convert_num_to_one_hot(label_tensor, num_labels=2):\n",
    "    one_hot_tensor = tf.one_hot(label_tensor, num_labels)\n",
    "    return tf.reshape(one_hot_tensor, [-1, num_labels])\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    ONE_HOT_FEATURES = {'Sex': 2, 'Embarked': 4}\n",
    "    TEXT_FEATURES = {'Name' : None}\n",
    "    FLOAT_FEATURE = {'Age': True, 'Fare': True}\n",
    "    LABEL_KEY = 'Survived'\n",
    "\n",
    "    outputs = {}\n",
    "    for key in ONE_HOT_FEATURES.keys():\n",
    "        dim = ONE_HOT_FEATURES[key]\n",
    "        index = tft.compute_and_apply_vocabulary(\n",
    "                fill_in_missing(inputs[key]), top_k=dim + 1)\n",
    "        outputs[transformed_name(key)] = convert_num_to_one_hot(\n",
    "                index, num_labels=dim + 1)\n",
    "\n",
    "    for key, to_norm in FLOAT_FEATURE.items():\n",
    "        if to_norm:\n",
    "            temp_feature = tft.scale_to_z_score(\n",
    "                                fill_in_missing(inputs[key])\n",
    "            )\n",
    "        else:\n",
    "            temp_feature = fill_in_missing(inputs[key])\n",
    "        outputs[transformed_name(key)] = temp_feature\n",
    "            \n",
    "    for key in TEXT_FEATURES.keys():\n",
    "        outputs[transformed_name(key)] = fill_in_missing(inputs[key])\n",
    "    outputs[transformed_name(LABEL_KEY)] = fill_in_missing(inputs[LABEL_KEY])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(root_dir, Config.PIPELINE_FOLDER)\n",
    "\n",
    "data_file = os.path.join(root_dir, 'data', \n",
    "                         'source_data', 'consumer_complaints_with_narrative.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-luxembourg",
   "metadata": {},
   "source": [
    "Now we're ready to start transforming our data in an Apache Beam pipeline.\n",
    "\n",
    "- Read in the data using the CSV reader\n",
    "- Transform it using a preprocessing pipeline that scales numeric data and converts categorical data from strings to int64 values indices, by creating a vocabulary for each category\n",
    "- Write out the result as a TFRecord of Example protos, which we will use for training a model later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-aerospace",
   "metadata": {},
   "source": [
    "what code does?\n",
    "\n",
    "1) **first of all we are going to read the data from the csv file using csv reader** <br>\n",
    "     \n",
    "Create a TFXIO to read the census data with the schema. To do this weneed to list all columns in order since the schema doesn't specify the order of columns in the csv. ```tfxio.CsvTFXIO``` can be used to both read the CSV files and parse them to TFT inputs<br><br>\n",
    " \n",
    "what if you can't able to read data directly with ```tfxio.CsvTFXIO``` because of some extra spaces? <br>\n",
    "&emsp;&emsp;The idea will be like, read CSV files as text using ```beam.io``` and then do some clensing to remove extra space and then pharse them to TFT using ```BeamRecordCsvTFXIO``` whose ```.BeamSource()``` accepts a PCollection[bytes] because we need to patch the records \n",
    "\n",
    "2) **Combine data and schema into a dataset tuple**<br>\n",
    "&emsp;&emsp;```Note: we already used the schema to read the CSV data, but we also need it to interpret raw_data.```\n",
    "\n",
    "3) **Apply transformation using ```tft_beam.AnalyzeAndTransformDataset```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-message",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    with tft_beam.Context(temp_dir = base_dir):\n",
    "        # read and pharse csv data\n",
    "        csv_tfxio = tfxio.CsvTFXIO(\n",
    "                    train_datafile, \n",
    "                    column_names = columns,\n",
    "                    telemetry_descriptors = ['Demo'],\n",
    "                    schema = SCHEMA,\n",
    "                    delimiter = '|',\n",
    "                    skip_header_lines = 1, skip_blank_lines= False)\n",
    "        raw_data = (p |\n",
    "                   'ReadTextFile' >> csv_tfxio.BeamSource()\n",
    "                   )\n",
    "        \n",
    "        # combine raw data and meta_data as tuple\n",
    "        raw_dataset = (raw_data, csv_tfxio.TensorAdapterConfig())\n",
    "        \n",
    "        # apply transfomation\n",
    "        transformed_dataset, transform_fn = (\n",
    "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "\n",
    "        transformed_data, transformed_metadata = transformed_dataset\n",
    "        \n",
    "        # coder to encode the transformed data before writing it as tfrecords\n",
    "        transformed_data_coder = tft.coders.ExampleProtoCoder(\n",
    "          transformed_metadata.schema)\n",
    "        \n",
    "        _ = (\n",
    "          transformed_data | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
    "                           | 'WriteTrainData' >> beam.io.WriteToTFRecord(OUPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "artifact_path = os.path.join(root_dir, Config.PIPELINE_FOLDER, 'tftransform_tmp')\n",
    "\n",
    "all_subdirs = [os.path.join(artifact_path,d) for d in os.listdir(artifact_path) if os.path.isdir(os.path.join(artifact_path,d))]\n",
    "\n",
    "latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
    "log_dir = os.path.join(root_dir, Config.TENSORBOARD_LOGGING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-huntington",
   "metadata": {},
   "source": [
    "Explore transform graph using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import load_graph_to_tensorboard\n",
    "\n",
    "load_graph_to_tensorboard(latest_subdir, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {log_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
