{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from utils.helper_metastore import *\n",
    "from utils.configurations.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "\n",
    "Tensorflow Data Validation (TFDV) can analyze training and serving data to:\n",
    "\n",
    "- [compute descriptive statistics](#Generating-Statistics)\n",
    "\n",
    "- [infer a schema](#Generating-schema)\n",
    "\n",
    "- [detect data anomalies](#Anomalies-detection)\n",
    "\n",
    "- [data skew and drift](#Data-Skew-and-Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'absl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_dir, 'data', 'dataset1')\n",
    "\n",
    "data_file = os.listdir(data_dir)[0]\n",
    "data_dir = os.path.join(data_dir,data_file)\n",
    "\n",
    "base_dir = os.path.join(root_dir, Config.PIPELINE_FOLDER)\n",
    "file = [i for i in os.listdir(base_dir) if 'sqlite' in i]\n",
    "config = os.path.join(base_dir, file[0])\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = config\n",
    "\n",
    "store = metadata_store.MetadataStore(connection_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_execution_status = get_latest_executions(store, Config.PIPELINE_NAME, ['CsvExampleGen', 'ImportExampleGen'])\n",
    "if previous_execution_status:\n",
    "    previous_execution_status = previous_execution_status[0].last_known_state\n",
    "else:\n",
    "    raise Exception('[Exception] Run the Data Ingestion Notebook before Running this...') \n",
    "    \n",
    "if  previous_execution_status == 3:\n",
    "    print('[INFO] previous component Execution State is Success. You can Proceed Further now..')\n",
    "elif previous_execution_status == 2:\n",
    "    print('[Warning] previous component Execution is in Running State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Statistics\n",
    "\n",
    "TFDV can compute descriptive statistics that provide a quick overview of the data in terms of the features that are present and their value distributions. It also provides an interactive visualization of those statistic by using [Facets](https://pair-code.github.io/facets/) tool.\n",
    "\n",
    "```generate_stistics_from_csv```  method is used to calculate statistic from the csv data file  from local or cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_csv(data_location = data_dir,\n",
    "                                         delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above visualization you can found many statistic which helps us to understand the distribution and charecteristics of the features. \n",
    "\n",
    "For numerical features, TFDV computes for every feature:\n",
    "- The overall count of data records\n",
    "- The number of missing data records\n",
    "- The mean and standard deviation of the feature across the data records\n",
    "- The minimum and maximum value of the feature across the data records\n",
    "- The percentage of zero values of the feature across the data records\n",
    "In addition, it generates a histogram of the values for each feature.\n",
    "\n",
    "For categorical features, TFDV provides:\n",
    "- The overall count of data records\n",
    "- The percentage of missing data records\n",
    "- The number of unique records\n",
    "- The average string length of all records of a feature\n",
    "- For each category, TFDV determines the sample count for each label and its rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating schema\n",
    "\n",
    "The schema describes the expected properties of the data which is used to detect errors during training or serving time. Some of these properties are:\n",
    "\n",
    "- which features are expected to be present\n",
    "- their type\n",
    "- the number of values for a feature in each example\n",
    "- the presence of each feature across all examples\n",
    "- the expected domains of features.\n",
    "\n",
    "e.g., several datasets can conform to the same schema, whereas statistics (described above) can vary per dataset.\n",
    "\n",
    "TFDV uses conservative heuristics to infer stable data properties from the statistics **in order to avoid overfitting the schema to the specific dataset**. It is strongly advised to review the inferred schema and refine it as needed, to capture any domain knowledge about the data that TFDV's heuristics might have missed.\n",
    "\n",
    ">note: These lines are taken from official [Tensorflow website](https://www.tensorflow.org/tfx/data_validation/get_started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualization, Presence means whether the feature must be present in 100% of\n",
    "data examples ( required ) or not ( optional ). Valency means the number of values\n",
    "required per training example. In the case of categorical features, single would mean\n",
    "each training example must have exactly one category for the feature.\n",
    "\n",
    "The schema that has been generated here may not be exactly what we need, it\n",
    "assumes that the current dataset is exact representation of future data as well. If a\n",
    "feature is present in all training examples in this dataset, it will be marked as\n",
    "required , but in reality it may be optional.\n",
    "\n",
    "so how can I update the schema stats based on domain knowledge?\n",
    "\n",
    "It is shown in session [Updating Schema](#Updating-schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "artifacts = display_artifacts(store, store.get_artifacts())\n",
    "uri = artifacts.uri[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "artifacts_prop = display_properties(store.get_artifacts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "split_names = artifacts_prop.loc[(artifacts_prop.name == 'split_names') & \n",
    "                   (artifacts_prop['artifact id'] == artifacts['artifact id'][0])].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "temp_store = {}\n",
    "for split in eval(split_names[0]):\n",
    "    file = os.path.join(uri, split)\n",
    "    file = os.path.join(file, os.listdir(file)[0])\n",
    "    temp_store[split] = tfdv.generate_statistics_from_tfrecord(\n",
    "                        data_location = file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "train_stats = temp_store['train']\n",
    "val_stats = temp_store['eval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing schema\n",
    "\n",
    "Let’s say we have two datasets: training and validation datasets. Before training our\n",
    "machine learning model, we would like to determine how representative the valida‐\n",
    "tion set is in regards to the training set. Does the validation data follow our training\n",
    "data schema? TFDV is there to help you out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(lhs_statistics=val_stats, rhs_statistics=train_stats,\n",
    "lhs_name='VAL_DATASET', rhs_name='TRAIN_DATASET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TFDV to check for selection bias using the statistics visualizations . For example, if our dataset contains Gender as a categorical feature, we can check that this is not biased toward the male category. In our dataset, we have State as a categorical feature. Ideally, the distribution of example counts across the different US states would reflect the relative population in each state.(e.g., Texas, in third place, has a larger popu‐\n",
    "lation than Florida in second place). If we find this type of bias in our data and we\n",
    "believe this bias may harm our model’s performance, we can go back and collect\n",
    "more data or over/undersample our data to get the correct distribution.\n",
    "\n",
    "| value | validation_data | training_data | \n",
    "| -- | -- | -- |\n",
    "| CA | 3408 | 6573 |\n",
    "| FL | 1946 | 4010 |\n",
    "| TX | 1906 | 3794 |\n",
    "\n",
    "click `show raw data` in the left corner of the chart in state feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalies detection\n",
    "\n",
    "TFDV itself detect some short of anomalies present in the data by using statistis and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = tfdv.validate_statistics(statistics=val_stats, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating schema\n",
    "\n",
    "The preceding anomaly protocol shows us how to detect variations from the schema\n",
    "that is autogenerated from our dataset. But another use case for TFDV is manually\n",
    "setting the schema according to our domain knowledge of the data.\n",
    "\n",
    "For example TFDV infers sub_issue feature will available in  80% of our examples.if we decide that we need to require this feature to be present in greater than 90% of our training examples, we can update the schema to reflect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_issue_feature = tfdv.get_feature(schema, 'sub_issue')\n",
    "sub_issue_feature.presence.min_fraction = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_domain = tfdv.get_domain(schema, 'state')\n",
    "state_domain.value.remove('AK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above action of removing AK from state domain is to show wheather TFDV detect AK is missed out in state domain list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_anomalies = tfdv.validate_statistics(val_stats, schema)\n",
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that state is maked with multiple errors and it also noted that AK is missed out in schema but which was presented in the dataset.\n",
    "\n",
    "so we can discus about this with our domain experts and add it manually.\n",
    "\n",
    "```\n",
    "state_domain = tfdv.get_domain(schema, 'state')\n",
    "state_domain.value.append('AK')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing and reading schema also be done for future data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_location = os.path.join(os.pardir, Config.PIPELINE_FOLDER, 'schema.pbtxt')\n",
    "\n",
    "tfdv.write_schema_text(schema, schema_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "tree ../temp_ -I *ExampleGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read schema file:\n",
    "\n",
    "```\n",
    "tfdv.read_schema_text(schema, schema_location)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Skew and Drift\n",
    "\n",
    "**Data Skew:**\n",
    "\n",
    "TFDV provides a built-in “skew comparator” that detects large differences between\n",
    "the statistics of two datasets. This isn’t the statistical definition of skew (a dataset that\n",
    "is asymmetrically distributed around its mean). It is defined in TFDV as the You can also adjust the schema so that different features are required in the training and serving environ‐ments. Data ValidationL-infinity norm of the difference between the serving_statistics of two datasets. If the difference between the two datasets exceeds the threshold of the L-infinity norm for a given feature, TFDV highlights it as an anomaly using the anomaly detection.\n",
    "\n",
    "\n",
    "**TYPES OF DRIFT'S**\n",
    "\n",
    "- **Concept drift** or change in P(Y|X) is a shift in the actual relationship between the model inputs and the output. \n",
    "- **Label drift** or change in P(Y Ground Truth) is a shift in the model’s output or label distribution\n",
    "- **Data drift** or change in P(X) is a shift in the model’s input data distribution. Data drift is one of the reasons model accuracy degrades over time. It is nothing but underlying statistical properties of the predictors change. If the variable changes it will affects the model performance.\n",
    "\n",
    "The best way to address this issue is to continuously monitoring the models. Based on past experiences, an estimate can be made as to when drift starts to creep in the model. Based on this, the model can be proactively re-developed itselft to avoid risks associated with drift.\n",
    "\n",
    "Causes of data drift include:\n",
    "\n",
    "- Upstream process changes, such as a sensor being replaced that changes the units of measurement from inches to centimeters.\n",
    "- Data quality issues, such as a broken sensor always reading 0.\n",
    "- Natural drift in the data, such as mean temperature changing with the seasons.\n",
    "- Change in relation between features, or covariate shift.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">L-infinity norm\n",
    "The L-infinity norm is an expression used to define the difference\n",
    "between two vectors (in our case, the serving statistics). The L-\n",
    "infinity norm is defined as the maximum absolute value of the vec‐\n",
    "tor’s entries.\n",
    "For example, if the two vector are the statistic of two different distribution, minimum value of L-infinity norm represents similarity between two distribution.\n",
    "\n",
    "we can also use many other methods like KL-divergence, Jensen-Shannon Divergence so on..\n",
    "\n",
    "In TFDV we can use L-infinity norm or Jensen-shannon-divergence base on our preference\n",
    "\n",
    "> ***NOTE To detect skew for numeric features, specify a jensen_shannon_divergence threshold instead of an infinity_norm threshold in the skew_comparator***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how it works, I set threshold value to 0.0001 (which is not an reasonable threshold).\n",
    "you can set this value base on your bussiness problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(schema,'company').skew_comparator.infinity_norm.threshold = 0.0001\n",
    "\n",
    "skew_anomalies = tfdv.validate_statistics(statistics=train_stats,\n",
    "                                        schema=schema,\n",
    "                                        serving_statistics=val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(skew_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that company feature is marked with an anomalie ***'High Linfty distance between training and serving'***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to this skew example, you should define your drift_comparator for the fea‐\n",
    "tures you would like to watch and compare. You can then call validate_statistics\n",
    "with the two dataset statistics as arguments, one for your baseline (e.g., yesterday’s\n",
    "dataset) and one for a comparison (e.g., today’s dataset):\n",
    "\n",
    "```\n",
    "tfdv.get_feature(schema,'company').drift_comparator.infinity_norm.threshold = 0.01\n",
    "\n",
    "drift_anomalies = tfdv.validate_statistics(statistics=train_stats_today,\n",
    "                                            schema=schema,\n",
    "                                            previous_statistics=train_stats_yesterday)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sclicing Dataset\n",
    "\n",
    "TFDV can be used to slice datasets on features of our choice to infer whether they are biased.\n",
    "The scenario in which a subtle way for bias to enter data is when data is missing. If data is not missing at random, it may be missing more frequently for one group of people within the dataset than for others. This can mean that when the final model is trained, its performance is worse for these groups.\n",
    "\n",
    "In this example, we’ll look at data from different US states. We can slice the data so\n",
    "that we only get statistics from California using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_data_validation.utils import slicing_util\n",
    "\n",
    "slice_fn1 = slicing_util.get_feature_value_slicer(\n",
    "            features={'state': [b'CA']})\n",
    "\n",
    "slice_options = tfdv.StatsOptions(slice_functions=[slice_fn1])\n",
    "slice_stats = tfdv.generate_statistics_from_csv(\n",
    "                            data_location=data_dir,\n",
    "                            stats_options=slice_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "\n",
    "def display_slice_keys(stats):\n",
    "    print(list(map(lambda x: x.name, slice_stats.datasets)))\n",
    "\n",
    "def get_sliced_stats(stats, slice_key):\n",
    "    for sliced_stats in stats.datasets:\n",
    "        if sliced_stats.name == slice_key:\n",
    "            result = statistics_pb2.DatasetFeatureStatisticsList()\n",
    "            result.datasets.add().CopyFrom(sliced_stats)\n",
    "            return result\n",
    "        print('Invalid Slice key')\n",
    "        \n",
    "def compare_slices(stats, slice_key1, slice_key2):\n",
    "    lhs_stats = get_sliced_stats(stats, slice_key1)\n",
    "    rhs_stats = get_sliced_stats(stats, slice_key2)\n",
    "    tfdv.visualize_statistics(lhs_stats, rhs_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_slices(slice_stats, 'state_CA', 'All Examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Environments\n",
    "\n",
    "By default, validations assume that all datasets in a pipeline adhere to a single schema. In some cases introducing slight schema variations is necessary, for instance features used as labels are required during training (and should be validated), but are missing during serving.\n",
    "\n",
    "Environments can be used to express such requirements. In particular, features in schema can be associated with a set of environments using default_environment, in_environment and not_in_environment.\n",
    "\n",
    "For example, if the company feature is being used as the label in training, but missing in the serving data. Without environment specified, it will show up as an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/serving_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "serving_data = pd.read_csv(data_dir, nrows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_datapath = os.path.join(os.pardir, 'data', 'serving_dataset', 'serving_data.csv')\n",
    "\n",
    "serving_data.drop('company', axis = 1, inplace = True)\n",
    "serving_data.to_csv(serving_datapath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_data_stat = tfdv.generate_statistics_from_csv(data_location = serving_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_anomalies = tfdv.validate_statistics(\n",
    "            serving_data_stat, schema)\n",
    "\n",
    "tfdv.display_anomalies(serving_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we got an anomaly as column dropped ('company'). we have to indicate that this column won't be available in serving environment. for that we have to maintain evinornment for schema and mark company column as not required in serving evironment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.default_environment.append('TRAINING')\n",
    "schema.default_environment.append('SERVING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(schema, 'company').not_in_environment.append('SERVING')\n",
    "\n",
    "serving_anomalies_with_env = tfdv.validate_statistics(\n",
    "    serving_data_stat, schema, environment='SERVING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(serving_anomalies_with_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedup validation process\n",
    "\n",
    "As we collect more data, the data validation becomes a more time-consuming step in our machine learning workflow. One way of reducing the time to perform the validation is by taking advantage of available cloud solutions. By using a cloud provider, we aren’t limited to the computation power of our laptop or on-premise computing resources.\n",
    "\n",
    "This is not shown in this notebook, to know how to take the advantage of Google clouds DataFlow \n",
    "[click here](https://www.tensorflow.org/tfx/data_validation/get_started#running_on_google_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating TFDV into Your Machine Learning Pipeline\n",
    "\n",
    "So far, all methods we have discussed can be used in a standalone setup. This can be\n",
    "helpful to investigate datasets outside of the pipeline setup.\n",
    "TFX provides a pipeline component called StatisticsGen , SchemaGen which accepts the output of the previous ExampleGen components as input and then performs the generation of statistics and Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading artifact from metadata store\n",
    "\n",
    "In the previous notebook (Data Ingestion) we had run ExampleGen with several configurations. The StatisticsGen and SchemaGen requires previous run artifacts (ExampleGen) as an input.\n",
    "\n",
    "In this section I shown how to load the previous run artifacts from the metadatastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.types import artifact_utils\n",
    "from tfx.types import standard_artifacts\n",
    "from tfx.types import channel_utils\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = get_latest_artifacts(store, Config.PIPELINE_NAME)\n",
    "example = find_latest_artifacts_by_type(store, artifacts,\n",
    "                                        standard_artifacts.Examples.TYPE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_artifacts(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of artifacts has to be converted into channels before passing it to the compents\n",
    "example_gen = channel_utils.as_channel(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFDV as Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen, SchemaGen, ExampleValidator\n",
    "from tfx.orchestration.experimental.interactive.interactive_context \\\n",
    "        import InteractiveContext\n",
    "\n",
    "pipeline_name = Config.PIPELINE_NAME\n",
    "base_root = os.path.split(os.getcwd())[0]\n",
    "pipeline_root = os.path.join(base_root, Config.PIPELINE_FOLDER)\n",
    "beam_args = [\n",
    "    '--runner=DirectRunner'\n",
    "]\n",
    "\n",
    "if not os.path.exists(pipeline_root):\n",
    "    raise Exception('Run Data Ingestion Notebook before running this')\n",
    "\n",
    "context = InteractiveContext(pipeline_name = pipeline_name,\n",
    "                            pipeline_root = pipeline_root,\n",
    "                            beam_pipeline_args = beam_args)\n",
    "\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen)\n",
    "\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=True)\n",
    "\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ExampleValidator component detects a misalignment in the dataset statistics\n",
    "or schema between the new and the previous dataset, it will set the status to failed in\n",
    "the metadata store, and the pipeline ultimately stops. Otherwise, the pipeline moves\n",
    "on to the next step, the data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_artifacts(store, store.get_artifacts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_artifacts(statistics_gen.outputs['statistics'].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_artifacts(schema_gen.outputs['schema'].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_artifacts(example_validator.outputs['anomalies'].get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    ">The ExampleValidator can automatically detect the anomalies\n",
    "against the schema by using the skew and drift comparators we\n",
    "described previously. However, this may not cover all the potential\n",
    "anomalies in your data. If you need to detect some other specific\n",
    "anomalies, you will need to write your own custom componen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
