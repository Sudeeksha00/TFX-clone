{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "western-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "from utils.helper_metastore import *\n",
    "from utils.configurations.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sapphire-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'absl')\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-refund",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-custom",
   "metadata": {},
   "source": [
    "Here comes the most important part of our pipeline (model training). The whole pipeline that we are trying to build on are jointly called as an Continous training component in the MLOps workflow. The main moto over here is to keep our model upto date in the production and the counter back the data drift, model drift and training-serving skew.\n",
    "\n",
    "![mlops pipeline](image/MLOps_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-bankruptcy",
   "metadata": {},
   "source": [
    "In this notebook, we cover the model training process as part of a machine learning pipeline, including how it is automated in a TFX pipeline. We also include some details of distribution strategies available in TensorFlow and how to tune hyperparameters in a pipeline. This chapter is more specific to TFX pipelines than most of the\n",
    "others because we don’t cover training as a standalone process.\n",
    "\n",
    "One very important feature of training a model in a TFX pipeline is that the data pre-processing step which ar saved along with the trained model weights. This is very useful once our model is deployed to production because it means that the preprocessing steps will always produce the features the models expecting. Without this feature, it would be possible to update the data pre‐processing steps without updating the model, and then the model would fail in production. So why TFX eports the proprocessing steps and the model as one graph, this potentialy eliminates the source of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cleared-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "import tensorflow as tf\n",
    "\n",
    "base_dir = os.path.join(root_dir, Config.PIPELINE_FOLDER)\n",
    "file = [i for i in os.listdir(base_dir) if 'sqlite' in i]\n",
    "config = os.path.join(base_dir, file[0])\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = config\n",
    "\n",
    "store = metadata_store.MetadataStore(connection_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "miniature-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] previous component Execution State is Success. You can Proceed Further now..\n"
     ]
    }
   ],
   "source": [
    "previous_execution_status = get_latest_executions(store, Config.PIPELINE_NAME, 'Transform')\n",
    "\n",
    "if previous_execution_status:\n",
    "    previous_execution_status = previous_execution_status[0].last_known_state\n",
    "else:\n",
    "    raise Exception('[Exception] Run the Data Ingestion Notebook before Running this...') \n",
    "    \n",
    "if  previous_execution_status == 3:\n",
    "    print('[INFO] previous component Execution State is Success. You can Proceed Further now..')\n",
    "elif previous_execution_status == 2:\n",
    "    print('[Warning] previous component Execution is in Running State')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-laugh",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-parking",
   "metadata": {},
   "source": [
    "In this session, let assume we created some model which was trained on offline experimentation. we then decided to productionize it with the MLOps worflow. In this pipeline step, we want to export the model with our preprocessing steps, we need to guarantee that the model input names match the transformed feature names from ```preprocessing_fn()```. In our example model, we reuse the ```transformed_name()``` function to add the suffix _xf to our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "functional-daughter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../script/module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../script/module.py\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "from utils.configurations.config import Config\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    LABEL_KEY = \"consumer_disputed\"\n",
    "    # Loop over the features and create an input for each feature.\n",
    "    # Feature name, feature dimensionality.\n",
    "    ONE_HOT_FEATURES = {\n",
    "    \"product\": 11,\n",
    "    \"sub_product\": 45,\n",
    "    \"company_response\": 5,\n",
    "    \"state\": 60,\n",
    "    \"issue\": 90\n",
    "    }\n",
    "\n",
    "    # Feature name, bucket count.\n",
    "    BUCKET_FEATURES = {\n",
    "    \"zip_code\": 10\n",
    "    }\n",
    "\n",
    "    # Feature name, value is unused.\n",
    "    TEXT_FEATURES = {\n",
    "    \"consumer_complaint_narrative\": None\n",
    "    }\n",
    "    # One-hot categorical features\n",
    "    input_features = []\n",
    "    for key, dim in ONE_HOT_FEATURES.items():\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(dim + 1,),\n",
    "            name=transformed_name(key)))\n",
    "        \n",
    "    # Adding bucketized features\n",
    "    for key, dim in BUCKET_FEATURES.items():\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(dim + 1,),\n",
    "            name=transformed_name(key)))\n",
    "    \n",
    "    # Adding text input features\n",
    "    input_texts = []\n",
    "    for key in TEXT_FEATURES.keys():\n",
    "        input_texts.append(\n",
    "            tf.keras.Input(shape=(1,),\n",
    "            name=transformed_name(key),\n",
    "            dtype=tf.string))\n",
    "    \n",
    "    inputs = input_features + input_texts\n",
    "    # Embed text features\n",
    "    # Load the tf.hub module of the Universal Sentence Encoder model.\n",
    "    MODULE_URL = Config.UNIVERSAL_EMBEDDING_MODEL\n",
    "    embed = hub.KerasLayer(MODULE_URL)\n",
    "    \n",
    "    reshaped_narrative = tf.reshape(input_texts[0], [-1]) # Keras inputs are two-dimensional, \n",
    "                                                            # but the encoder expects one-dimensional inputs.\n",
    "    embed_narrative = embed(reshaped_narrative)\n",
    "    deep_ff = tf.keras.layers.Reshape((512, ), input_shape=(1, 512))(embed_narrative)\n",
    "    deep = tf.keras.layers.Dense(256, activation='relu')(deep_ff)\n",
    "    deep = tf.keras.layers.Dense(64, activation='relu')(deep)\n",
    "    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n",
    "    wide_ff = tf.keras.layers.concatenate(input_features)\n",
    "    wide = tf.keras.layers.Dense(16, activation='relu')(wide_ff)\n",
    "    both = tf.keras.layers.concatenate([deep, wide])\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(both)\n",
    "    keras_model = tf.keras.models.Model(inputs, output)\n",
    "    keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=[\n",
    "                        tf.keras.metrics.BinaryAccuracy(),\n",
    "                        tf.keras.metrics.TruePositives()\n",
    "                    ])\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-shape",
   "metadata": {},
   "source": [
    "## TFX Trainer Component\n",
    "\n",
    "The TFX Trainer component handles the training step in our pipeline. the Trainer component will produce a model that will be put into production.The Transform steps will be included in this model, the data preprocessing steps will always match what the model is expecting. This removes a huge potential source of errors when our model is deployed.\n",
    "\n",
    "In our example project, the Trainer component requires the following inputs:\n",
    "- The previously generated data schema, generated by the data validation step\n",
    "- The transformed data and its preprocessing graph\n",
    "- Training parameters (e.g., the number of training steps)\n",
    "- A module file containing a run_fn() function, which defines the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-teaching",
   "metadata": {},
   "source": [
    "### run_fn() Function\n",
    "\n",
    "The Trainer component will look for a run_fn() function in our module file and use the function as an entry point to execute the training process. The module file needs The TFX Trainer Component to be accessible to the Trainer component. If we run the component in an interactive context, we can simply define the absolute path to the module file and pass it to the component. If you run your pipelines in production it will differ. It will be convered in the later notebook.\n",
    "\n",
    "The run_fn() function is a generic entry point to the training steps and not tf.Keras specific. It carries out the following steps:\n",
    "- Loading the training and validation data (or the data generator)\n",
    "- Defining the model architecture and compiling the model\n",
    "- Training the model\n",
    "- Exporting the model to be evaluated in the next pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "driven-surprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ../script/module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../script/module.py -a\n",
    "\n",
    "\n",
    "def run_fn(fn_args):\n",
    "    print(fn_args)\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output) # line 1\n",
    "    train_dataset = input_fn(fn_args.train_files, tf_transform_output) # line 2\n",
    "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output) # line 3\n",
    "    model = get_model() # line 4\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch') # line 5\n",
    "    model.fit( \n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard_callback])  # line 6\n",
    "    \n",
    "    signatures = {\n",
    "    'serving_default':\n",
    "    get_serve_tf_examples_fn(model,tf_transform_output).get_concrete_function(\n",
    "            tf.TensorSpec(\n",
    "            shape=[None],\n",
    "            dtype=tf.string,\n",
    "            name='examples')\n",
    "            )\n",
    "    } # line 7\n",
    "\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures) # line 8\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-affect",
   "metadata": {},
   "source": [
    "Line 1, 2, 3\n",
    "\n",
    "The ```run_fn``` function receives a set of arguments, including the transform graph, example datasets, and training parameters through the fn_args object.\n",
    "\n",
    "Loading data for model training and validation is performed in batches, and the loading is handled by the ```input_fn()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "historical-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ../script/module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../script/module.py -a\n",
    "\n",
    "LABEL_KEY = 'consumer_disputed'\n",
    "def _gzip_reader_fn(filenames):\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "\n",
    "def input_fn(file_pattern,\n",
    "    tf_transform_output, batch_size=32):\n",
    "    transformed_feature_spec = (\n",
    "    tf_transform_output.transformed_feature_spec().copy())\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=_gzip_reader_fn,\n",
    "        label_key=transformed_name(LABEL_KEY))\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-foundation",
   "metadata": {},
   "source": [
    "The input_fn function lets us load the compressed, preprocessed datasets that were generated by the previous Transform step. To do this, we need to pass the tf_transform_output to the function. This gives us the data schema to load the dataset from the TFRecord data structures generated by the Transform component. By using the\n",
    "preprocessed datasets, we can avoid data preprocessing during training and speed up the training process.\n",
    "\n",
    "\n",
    "The ```input_fn``` returns a generator which created using the ```batched_features_dataset``` function that will supply data to the model one batch at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-indiana",
   "metadata": {},
   "source": [
    "**<u>line 4, 6</u>**\n",
    "\n",
    "Now that we have defined our data-loading steps, the next step is defining our model architecture and compiling our model. In our run_fn , this will require a call to get_model() (line 4),\n",
    "\n",
    "\n",
    "Next, we train our compiled tf.Keras model with the Keras method fit() (line 6)\n",
    "\n",
    "Once the model training is complete, the next step is to export the trained model. we want to define how the preprocessing steps has to be exported with the model.\n",
    "\n",
    "In line 7, we define the model signature and saved the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-genealogy",
   "metadata": {},
   "source": [
    "The ```run_fn()``` exports the ```get_serve_tf_examples_fn``` as part of the model signature. When a model has been exported and deployed, every prediction request will pass through the ```serve_tf_examples_fn()```\n",
    "\n",
    "With every request, we parse the serialized tf.Example records and apply the preprocessing steps to the raw\n",
    "request data. The model then makes a prediction on the preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-mileage",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sorted-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ../script/module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../script/module.py -a\n",
    "\n",
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer() # Load the preprocessing graph\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        # Parse the raw tf.Example records from the request.\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        # Apply the preprocessing transformation to raw data.\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        # Perform prediction with preprocessed data.\n",
    "        outputs = model(transformed_features)\n",
    "        return {'outputs': outputs}\n",
    "    return serve_tf_examples_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-pressing",
   "metadata": {},
   "source": [
    "## Running the Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gross-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.types import channel_utils\n",
    "from tfx.types import standard_artifacts\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.components import Trainer\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.proto import trainer_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upper-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = get_latest_artifacts(store, Config.PIPELINE_NAME, 'SchemaGen')\n",
    "example_schema = find_latest_artifacts_by_type(store, artifacts, standard_artifacts.Schema.TYPE_NAME)\n",
    "example_schema = channel_utils.as_channel(example_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "timely-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = get_latest_artifacts(store, Config.PIPELINE_NAME, 'Transform')\n",
    "transform_examples = find_latest_artifacts_by_type(store, artifacts, standard_artifacts.Examples.TYPE_NAME)\n",
    "transform_examples = channel_utils.as_channel(transform_examples)\n",
    "\n",
    "transform_graph = find_latest_artifacts_by_type(store, artifacts, standard_artifacts.TransformGraph.TYPE_NAME)\n",
    "transform_graph = channel_utils.as_channel(transform_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lucky-interface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:From <ipython-input-12-feada9b2400c>:6: The name tfx.components.base.executor_spec.ExecutorClassSpec is deprecated. Please use tfx.dsl.components.base.executor_spec.ExecutorClassSpec instead.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_STEPS = 1000\n",
    "EVALUATION_STEPS = 100\n",
    "\n",
    "trainer = Trainer(\n",
    "    module_file=\"../script/module.py\",\n",
    "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "    transformed_examples=transform_examples,\n",
    "    transform_graph=transform_graph,\n",
    "    schema=example_schema,\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at /home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/metadata.sqlite.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FnArgs(working_dir=None, train_files=['/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Transform/transformed_examples/10/train/*'], eval_files=['/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Transform/transformed_examples/10/eval/*'], train_steps=1000, eval_steps=100, schema_path='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/SchemaGen/schema/8/schema.pbtxt', schema_file='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/SchemaGen/schema/8/schema.pbtxt', transform_graph_path='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Transform/transform_graph/10', transform_output='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Transform/transform_graph/10', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7fcdddf97a60>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7fcdddfe94c0>), serving_model_dir='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Trainer/model/32/serving_model_dir', eval_model_dir='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Trainer/model/32/eval_model_dir', model_run_dir='/home/jagan-ds/Documents/Tensorflow-Extended-tutorial/temp_/Trainer/model_run/32', base_model=None, hyperparameters=None, custom_config=None)\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = Config.PIPELINE_NAME\n",
    "base_root = os.path.split(os.getcwd())[0]\n",
    "pipeline_root = os.path.join(base_root, f'temp_')\n",
    "beam_args = [\n",
    "    '--runner=DirectRunner'\n",
    "]\n",
    "\n",
    "if not os.path.exists(pipeline_root):\n",
    "    raise Exception('Please do follow the notebook sequence')\n",
    "\n",
    "context = InteractiveContext(pipeline_name = pipeline_name,\n",
    "                            pipeline_root = pipeline_root,\n",
    "                            beam_pipeline_args = beam_args)\n",
    "\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-chase",
   "metadata": {},
   "source": [
    "After the model training and exporting is completed, the component will register the path of the exported model with the metadata store. Downstream components can pick up the model for the model validation.\n",
    "The Trainer component is generic and not limited to running TensorFlow models. However, the components later in the pipeline expect that the model is saved in the TensorFlow SavedModel format. The SavedModel graph includes the Transform graph, so the data preprocessing steps are part of the model now.\n",
    "\n",
    ">**note:**<br>\n",
    ">> ***Overriding the Trainer Component’s Executor***<br>\n",
    "we override the Trainer component’s executor to enable the generic training entry point ```run_fn()``` function instead of the default ```trainer_fn()``` function, which only supports ```tf.Estimator``` models. In later notebook, we will see another Trainer executor, the ```ai_platform_trainer_executor.GenericExecutor```. This executor allows you to train models on Google Cloud’s AI Platform instead of inside your pipeline. This is an alternative if your model requires specific training hardware (e.g., GPUs or tensor processing units [TPUs]), which aren’t available in your pipeline environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-cooler",
   "metadata": {},
   "source": [
    "To view TensorBoard in a notebook, we get the location of the model training logs and pass it to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-edwards",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_dir = trainer.outputs['model'].get()[0].uri\n",
    "\n",
    "%tensorboard --logdir {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-knowing",
   "metadata": {},
   "source": [
    "### Using the SavedModel outside a pipeline\n",
    "\n",
    "If we would like to inspect the exported SavedModel outside a TFX pipeline, we can load the model as a concrete function, which represents the graph of a single signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = trainer.outputs.model.get()[0].uri\n",
    "model_path = os.path.join(model_path, 'serving_model_dir')\n",
    "model = tf.saved_model.load(export_dir=model_path)\n",
    "predict_fn = model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-lawsuit",
   "metadata": {},
   "source": [
    "With the model loaded as a concrete function, we can now perform predictions. The exported model expects the input data to be provided in the tf.Example data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    \"product\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['Debt collection'.encode()])),\n",
    "                    \"sub_product\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['I do not know'.encode()])),\n",
    "                    \"issue\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['Disclosure verification of debt'.encode()])),\n",
    "                    \"sub_issue\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['Right to dispute notice not received'.encode()])),\n",
    "                    \"state\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['NY'.encode()])),\n",
    "                    \"zip_code\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['113XX'.encode()])),\n",
    "                    \"company\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['Encore Capital Group'.encode()])),\n",
    "                    \"company_response\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['Closed with explanation'.encode()])),\n",
    "                    \"timely_response\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['Yes'.encode()])),\n",
    "                    \"consumer_complaint_narrative\": tf.train.Feature(bytes_list=tf.train.BytesList(value=['I was denied employment because of a judgment against me.  I was/ am completely unaware of any hearing, never received any notice of collection of a debt by Midland LLC .  Midland LLC apparently took me to court somewhere without serving me any documents, and won a courts judgement.  I was never notified of any hearing, complaint, or received notice of collection of a debt by Midland LLC.'.encode()]))\n",
    "}))\n",
    "\n",
    "serialized_example = example.SerializeToString()\n",
    "prediction = predict_fn(tf.constant([serialized_example]))['outputs'].numpy()[0,0]\n",
    "print(f'predicted probability : {prediction}')\n",
    "print(f'predicted value : {int(prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-gentleman",
   "metadata": {},
   "source": [
    "## Distribution Strategies\n",
    "\n",
    "We done with the trainer component what if our model needs more computation power to be utilized to proceed training faseter?\n",
    "\n",
    "Therefore, TensorFlow provides distribution strategies for machine learning models that can’t be adequately trained on a single GPU. You might want to consider distribution strategies when you want to accelerate your training or you can’t fit the entire model into a single GPU. The strategies we describe here are abstractions to distribute the model parameters across multiple GPUs or even multiple servers. In general, there are two groups of\n",
    "strategies: \n",
    "- synchronous\n",
    "- asynchronous\n",
    "\n",
    "Under the synchronous strategies, all training workers train with different slices of the training data synchronously and then aggregate the gradients from all workers before updating the model. \n",
    "\n",
    "The asynchronous strategies train models independently with the entire dataset on differentworkers. Each worker updates the gradients of the model asynchronously, without waiting for the other workers to finish. \n",
    "\n",
    "Typically, synchronous strategies are coordinated  via all-reduce operations and asynchronous strategies through a parameter server architecture. A few synchronous and asynchronous strategies exist, and they have their benefits\n",
    "and drawbacks.\n",
    "\n",
    "Types of statergies in tensorflow are:\n",
    "- MirroredStrategy\n",
    "- TPUStrategy\n",
    "- MultiWorkerMirroredStrategy\n",
    "- CentralStorageStrategy\n",
    "- ParameterServerStrategy\n",
    "\n",
    "### MirroredStrategy\n",
    "```tf.distribute.MirroredStrategy``` supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Together, these variables form a single conceptual variable called MirroredVariable. These variables are kept in sync with each other by applying identical updates.\n",
    "\n",
    "Efficient all-reduce algorithms are used to communicate the variable updates across the devices. All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device.\n",
    "\n",
    "\n",
    "### TPUStrategy\n",
    "```tf.distribute.TPUStrategy``` lets you run your TensorFlow training on Tensor Processing Units (TPUs). TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. They are available on Google Colab, the TensorFlow Research Cloud and Cloud TPU.\n",
    "\n",
    "In terms of distributed training architecture, TPUStrategy is the same ```MirroredStrategy```, it implements synchronous distributed training. TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores, which are used in TPUStrategy.\n",
    "\n",
    "\n",
    "### MultiWorkerMirroredStrategy\n",
    "```tf.distribute.MultiWorkerMirroredStrategy``` is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "\n",
    "\n",
    "### ParameterServerStrategy\n",
    "Parameter server training is a common data-parallel method to scale up model training on multiple machines. A parameter server training cluster consists of workers and parameter servers. Variables are created on parameter servers and they are read and updated by workers in each step.\n",
    "\n",
    "\n",
    "### CentralStorageStrategy\n",
    "```tf.distribute.experimental.CentralStorageStrategy``` does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.\n",
    "\n",
    "![distribution_stratergy](image/distribution_stratergy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-panama",
   "metadata": {},
   "source": [
    "An example of it here. We can apply the MirroredStrategy easily by adding a few lines before\n",
    "invoking our model creation and the subsequent model.compile() call.\n",
    "\n",
    "for that replace the ```model=get_model()``` (line 4) in ```run_n()``` with the code below \n",
    "```\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "model = get_model()\n",
    "```\n",
    "\n",
    "This will help us the train our model in a distributed setup. we wrap the model creation and compilation with the Python manager (in our case, it all happens inside of the get_model() function). This will create and compile our model under the distribution scope of our choice. The ```MirroredStrategy``` will use all available GPUs of the instance. If you want to reduce the number of GPU instances being used, you can specify the GPUs to be used with the MirroredStrategy by changing the creation of the distribution strategy:\n",
    "\n",
    "```\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"]\n",
    "```\n",
    "\n",
    "In this example, we specify two GPUs to be used for our training runs. These distribution strategies are useful for large training jobs that won’t fit on the memory of a single GPU.\n",
    "\n",
    "> note: <br>\n",
    "Batch Size Requirement When Using the MirroredStrategy.The MirroredStrategy expects that the batch size is proportional to the number of devices. For example, if you train with five GPUs, the batch size needs to be a multiple of the number of GPUs. Please keep this in mind when you set up your input_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-sandwich",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning in TFX Pipelines\n",
    "In a TFX pipeline, hyperparameter tuning takes in the data from the Transform com‐\n",
    "ponent and trains a variety of models to establish the best hyperparameters. The\n",
    "hyperparameters are then passed to the Trainer component, which then trains a final\n",
    "model using them.\n",
    "In this case, the model definition function (the get_model function in our example)\n",
    "needs to accept the hyperparameters as an input and build the model according to\n",
    "the specified hyperparameters. So, for example, the number of layers needs to be\n",
    "defined as an input argument."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
