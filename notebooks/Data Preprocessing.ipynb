{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "from utils.helper_metastore import *\n",
    "from utils.configurations.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'absl')\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-trading",
   "metadata": {},
   "source": [
    "## General Introduction\n",
    "\n",
    "The data that we used to train our machine learning model is not always available in required format. \n",
    "\n",
    ">For eg, target variable in the dataset that we explored before is categorical (Yes or No) but machine learning model prefers only numeric data. \n",
    "\n",
    "Also we have to do some feature engineering in the input featuresto imporove the model accurac.\n",
    ">For eg, Feature Crossing etc..\n",
    "\n",
    "For all these process we need some function which has to apply all these transformation to the input features both on training and serving time.\n",
    "\n",
    "\n",
    "\n",
    "As a Data Scientist, we mostly prefer python notebook with pandas or in Spark jobs to develop initial process like Data Exploration, creating preprocessing functions and model development. We should re-use the same preprocessing code to guarantee that a given raw input maps to the same feature vector at training and serving time. If this does not happen, we have training-serving skew.\n",
    "\n",
    "Let imagine you had asigned to new ml project, you decided to use spark pipeline to process raw data. In the production server the preprocessing steps are\n",
    "implemented in an API. Due to some limitations you are forced to re-implement all your preprocessing steps using numpy/pandas. Now you have to maintain two different implementation setups for preprocessing(spark and numpy/pandas). Given an input, you must ensure they are giving same output to avoid training-serving skew.\n",
    "![Figure_1](image/data_preprocessing_fig_1.png)\n",
    "\n",
    "With TFT, we can avoid a misalignment of the preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-pearl",
   "metadata": {},
   "source": [
    "## Why tensorflow transform?\n",
    "\n",
    "Three main purpose of Tensorflow Transforms are:\n",
    "- Preprocessing your data efficiently in the context of the entire dataset\n",
    "- Scaling the preprocessing steps effectively\n",
    "- Avoiding a potential training-serving skew\n",
    "\n",
    "TFT processes the data that we ingested into our pipeline with the earlier generated\n",
    "dataset schema, and it outputs two artifacts:\n",
    "- Preprocessed training and evaluation datasets in the TFRecord format. The produced datasets can be consumed downstream in the Trainer component of our pipeline.\n",
    "- Exported preprocessing graph (with assets), which will be used when we’ll export our machine learning model.\n",
    "\n",
    "The key to TFT is the preprocessing_fn function. The function defines all transformations we want to apply to the raw data. When we execute the Transform component, the preprocessing_fn function will receive the raw data,\n",
    "apply the transformation, and return the processed data. The data is provided as TensorFlow Tensors or SparseTensors (depending on the feature). All transformations applied to the tensors have to be TensorFlow operations. This allows TFT to effectively distribute the preprocessing steps.\n",
    "\n",
    "TFT uses Apache Beam under the hood to execute preprocessing instructions. This allows us to distribute the preprocessing if needed on the Apache Beam backend of our choice. If you don’t have access to Google Cloud’s Dataflow product or an Apache Spark or Apache Flink cluster, Apache Beam will default back to its Direct\n",
    "Runner mode. \n",
    "\n",
    ">Note:\n",
    "To avoid a misalignment between the preprocessing steps and the trained model, the exported model can include the preprocessing graph and the trained model. We can then deploy the model like any other TensorFlow model, but during our inference, the data will be preprocessed on the model server as part of the model inference. This avoids the requirement that preprocessing happen on the client side and simplifies the development of clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-pursuit",
   "metadata": {},
   "source": [
    "## Defining Preprocessing function\n",
    "\n",
    "TFT accepts preprocessing function and it will outputs the tf graph and transformed dataset.The preprocessing function must accepts and returns a dictionary of tensors. We can use two type of function to define preprocessing\n",
    "function: \n",
    "\n",
    "- One will be the function which can accepts and returns tensors. These add operation to the tf graph that transform rat data into transformed data.\n",
    "- Another will be the analyzer functions(tft.min, tft.scale_to_z_score, etc.,) provided by tf.Transform, it to accept and return tensors but unlike tensorflow functions, they won't be added as an operation to the graph. Instead they will compute a full pass operation outside of TensorFlow.They use the input tensor values over the entire dataset to generate a constant tensor that is returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx.components import Transform\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "base_dir = os.path.join(root_dir, Config.PIPELINE_FOLDER)\n",
    "file = [i for i in os.listdir(base_dir) if 'sqlite' in i]\n",
    "config = os.path.join(base_dir, file[0])\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = config\n",
    "\n",
    "store = metadata_store.MetadataStore(connection_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_execution_status_stat = get_latest_executions(store, Config.PIPELINE_NAME, 'StatisticsGen')\n",
    "previous_execution_status_schema = get_latest_executions(store, Config.PIPELINE_NAME, 'SchemaGen')\n",
    "if previous_execution_status_stat and previous_execution_status_schema:\n",
    "    previous_execution_status_schema = previous_execution_status_schema[0].last_known_state\n",
    "    previous_execution_status_stat = previous_execution_status_stat[0].last_known_state\n",
    "else:\n",
    "    raise Exception('[Exception] Run the Data Ingestion Notebook before Running this...') \n",
    "    \n",
    "if  previous_execution_status_schema == 3 and previous_execution_status_stat == 3:\n",
    "    print('[INFO] previous component Execution State is Success. You can Proceed Further now..')\n",
    "elif previous_execution_status_schema == 2 and previous_execution_status_stat == 3:\n",
    "    print('[Warning] SchemaGen Component Execution is in Running State')\n",
    "elif previous_execution_status_schema == 3 and previous_execution_status_stat == 2:\n",
    "    print('[Warning] StatisticsGen Component Execution is in Running State')\n",
    "elif previous_execution_status_stat == 2 and previous_execution_status_schema == 2:\n",
    "    print('[Warning] previous component Execution is in Running State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    x1 = inputs['x1']\n",
    "    x2 = inputs['x2']\n",
    "    x1_normalized = tft.scale_to_0_1(x1)\n",
    "    x2_integerized = tft.compute_and_apply_vocabulary(x2)\n",
    "    return {\n",
    "      'x1_normalized': x1_normalized,\n",
    "      'x2_integerized': x2_integerized\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-desire",
   "metadata": {},
   "source": [
    "```Preprocessing_fn``` will accept and return dictionary. Our dummy dataset will contain two variables \n",
    "> x1 => dtype: int; x2 => dtype: str\n",
    "\n",
    "Our function will create four new tensors:\n",
    "- The second new tensor, x1_normalized, is created in a similar manner but using the convenience method tft.scale_to_0_1. This method does something similar to computing x_centered, namely computing a maximum and minimum and using these to scale y.\n",
    "- The tensor x2_integerized shows an example of string manipulation. In this case, we take a string and map it to an integer. This uses the convenience function tft.compute_and_apply_vocabulary. This function uses an analyzer to compute the unique values taken by the input strings, and then uses TensorFlow operations to convert the input strings to indices in the table of unique values.\n",
    "\n",
    "The preprocessing function defines a pipeline of operations on a dataset. The typical workflow of a tf.Transform user will construct a preprocessing function, then incorporate this into a larger Beam pipeline, creating the data for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "raw_data = [\n",
    "    {'x1': 1, 'x2': 'hello'},\n",
    "    {'x1': 2, 'x2': 'world'},\n",
    "    {'x1': 3, 'x2': 'hello'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-particle",
   "metadata": {},
   "source": [
    "### Data Formats and Schema\n",
    "\n",
    "TFT Beam implementation accepts two different input data formats. The \"instance dict\" format is suitable for small datasets while the TFXIO format provides improved performance and is suitble for large datasets.\n",
    "\n",
    "- If raw_data_metadata is a dataset_metadata.DatasetMetadata, then raw_data is expected to be in the \"instance dict\" format.\n",
    "- If raw_data_metadata is a tfxio.TensorAdapterConfig, then raw_data is expected to be in the TFXIO format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-pioneer",
   "metadata": {},
   "source": [
    "#### Instance dict fomat\n",
    "\n",
    "The metadata contains the schema which used to defines the layout of the data so it can be read and written to various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        'x1': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x2': tf.io.FixedLenFeature([], tf.string),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-english",
   "metadata": {},
   "source": [
    "The Schema proto contains the information needed to parse the data from its on-disk or in-memory format, into tensors. It is typically constructed by calling schema_utils.schema_from_feature_spec with a dict mapping feature keys to ```tf.io.FixedLenFeature```, ```tf.io.VarLenFeature``` and ```tf.io.SparseFeature``` values.\n",
    "\n",
    "Here we used tf.io.FixedLenFeature to indicate that each feature contains a fixed number of values, in this case a single scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-yemen",
   "metadata": {},
   "source": [
    "#### TFXIO format\n",
    "\n",
    "With this format, the data is expected to be contained in a pyarrow.RecordBatch. For tabular data, our Apache Beam implementation accepts Arrow RecordBatches that consist of columns of the following types:\n",
    "\n",
    "- pa.list_(<primitive>), where <primitive> is pa.int64(), pa.float32() pa.binary() or pa.large_binary().\n",
    "\n",
    "- pa.large_list(<primitive>)\n",
    "\n",
    "The toy input dataset we used above, when represented as a RecordBatch, looks like the following:\n",
    "\n",
    "```raw_data = [\n",
    "    pa.record_batch([\n",
    "        pa.array([[1], [2], [3]], pa.list_(pa.float32())),\n",
    "        pa.array([[1], [2], [3]], pa.list_(pa.float32())),\n",
    "        pa.array([['hello'], ['world'], ['hello']], pa.list_(pa.binary())),\n",
    "    ], ['x', 'y', 's'])\n",
    "]\n",
    "```\n",
    "Similar to DatasetMetadata being needed to accompany the \"instance dict\" format, a tfxio.TensorAdapterConfig is needed to accompany the RecordBatches. It consists of the Arrow schema of the RecordBatches, and TensorRepresentations to uniquely determine how columns in RecordBatches can be interpreted as TensorFlow Tensors (including but not limited to tf.Tensor, tf.SparseTensor).\n",
    "\n",
    "TensorRepresentations is a Dict[Text, TensorRepresentation] which establishes the relationship between a Tensor that preprocessing_fn accepts and columns in the RecordBatches. For example:\n",
    "\n",
    "\n",
    "```tensor_representation = {\n",
    "    'x': text_format.Parse(\n",
    "        \"\"\"dense_tensor { column_name: \"col1\" shape { dim { size: 2 } } }\"\"\"\n",
    "        schema_pb2.TensorRepresentation())\n",
    "}\n",
    "```\n",
    "Means that inputs['x'] in preprocessing_fn should be a dense tf.Tensor, whose values come from a column of name 'col1' in the input RecordBatches, and its (batched) shape should be [batch_size, 2].\n",
    "\n",
    "TensorRepresentation is a Protobuf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-brazil",
   "metadata": {},
   "source": [
    "### Executing preprocessing function\n",
    "\n",
    "The preprocessing function is an logical description of a preprocessing pipeline implemented on multiple data processing frameworks, tf.Transform provides a canonical implementation used on Apache Beam. This implementation demonstrates the functionality required from an implementation.\n",
    "\n",
    "In TXF, analysis step will extract constant values from data and the transform step will use those constant values to make calculations with batches of data’.\n",
    "\n",
    "\n",
    "![figure_2](image/data_preprocessing_fig_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tft_beam.Context(temp_dir=base_dir):\n",
    "    transformed_dataset, transform_fn = (\n",
    "        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn))\n",
    "\n",
    "    transformed_data, transformed_metadata = transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-effectiveness",
   "metadata": {},
   "source": [
    "what if we need to read data from the disc?\n",
    "\n",
    "Apache Beam provides functions to handle file ingestions effectively (e.g., with beam.io.Read\n",
    "FromText() or beam.io.ReadFromTFRecord() ) in the context of building TensorFlow\n",
    "models.\n",
    "\n",
    "As you can see Apache Beam executions can get complex quickly, and the data scientists and machine learning engineers aren’t in the business of writing execution instructions from scratch. This is why TFX is so handy. It abstracts all the instructions under the hood and lets the data scientist focus on their problem-specific setups like defining the preprocessing_fn() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n",
    "print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-scroll",
   "metadata": {},
   "source": [
    "#### What are artifacts are created?\n",
    "\n",
    "- transformed_dataset\n",
    "- transformed_metadata\n",
    "- transform_fn\n",
    "\n",
    "```transform_fn``` is a pure function that represents an operation that is applied to each row of the dataset. In particular, the analyzer values are already computed and treated as constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree ../temp_/tftransform_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-booking",
   "metadata": {},
   "source": [
    "#### how the transform graph looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import session\n",
    "from tensorflow.python.framework import importer\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.summary import summary\n",
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "try:\n",
    "    from tensorflow.contrib.tensorrt.ops.gen_trt_engine_op import *\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import load_graph_to_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(transform_fn[0][0])\n",
    "log_dir = os.path.join(root_dir, Config.TENSORBOARD_LOGGING)\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-reducing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_graph_to_tensorboard(model_dir, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-symposium",
   "metadata": {},
   "source": [
    "Try to explore what are the nodes are added into the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-score",
   "metadata": {},
   "source": [
    "### Integrate TFT into Your Machine Learning Pipeline\n",
    "\n",
    "Earlier, we had investigated the dataset and determined which features are categorical or numerical and we had explored the dataset using DataValidation Framework. This\n",
    "information is crucial for defining our feature engineering.\n",
    "\n",
    "In the following code, We are going to define a function which will apply transformation in the given features. Those newly generated feature are going to be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_KEY = \"consumer_disputed\"\n",
    "\n",
    "# Feature name, feature dimensionality.\n",
    "ONE_HOT_FEATURES = {\n",
    "\"product\": 11,\n",
    "\"sub_product\": 45,\n",
    "\"company_response\": 5,\n",
    "\"state\": 60,\n",
    "\"issue\": 90\n",
    "}\n",
    "\n",
    "# Feature name, bucket count.\n",
    "BUCKET_FEATURES = {\n",
    "\"zip_code\": 10\n",
    "}\n",
    "\n",
    "# Feature name, value is unused.\n",
    "TEXT_FEATURES = {\n",
    "\"consumer_complaint_narrative\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-donna",
   "metadata": {},
   "source": [
    "#### Prefrocessing_func\n",
    "\n",
    "With all the helper functions in place, we can now loop over each feature column and\n",
    "transform it depending on the type. For example, for our features to be converted to\n",
    "one-hot features, we convert the category names to an index with tft.com\n",
    "pute_and_apply_vocabulary() and then convert the index to a one-hot vector rep‐\n",
    "resentation with our helper function convert_num_to_one_hot() . Since we are using\n",
    "tft.compute_and_apply_vocabulary() , TensorFlow Transform will first loop over\n",
    "all categories and then determine a complete category to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {os.path.join(root_dir, Config.TRANSFORM_MODULE_SCRIPT)}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "LABEL_KEY = \"consumer_disputed\"\n",
    "\n",
    "# Feature name, feature dimensionality.\n",
    "ONE_HOT_FEATURES = {\n",
    "\"product\": 11,\n",
    "\"sub_product\": 45,\n",
    "\"company_response\": 5,\n",
    "\"state\": 60,\n",
    "\"issue\": 90\n",
    "}\n",
    "\n",
    "# Feature name, bucket count.\n",
    "BUCKET_FEATURES = {\n",
    "\"zip_code\": 10\n",
    "}\n",
    "\n",
    "# Feature name, value is unused.\n",
    "TEXT_FEATURES = {\n",
    "\"consumer_complaint_narrative\": None\n",
    "}\n",
    "\n",
    "# It is a good practice to rename the features by appending a suffix to the feature name\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "# Some of our features are of a sparse nature, but TFT expects the transformation outputs\n",
    "# to be dense\n",
    "# Following function will be used to convert the sparse into dense tensor\n",
    "def fill_in_missing(x):\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    if type(x) == tf.SparseTensor:\n",
    "        x = tf.sparse.to_dense(\n",
    "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "            default_value)\n",
    "    if len(x.shape) > 1:\n",
    "        x = tf.squeeze(x, axis = 1)\n",
    "    return x\n",
    "\n",
    "def convert_num_to_one_hot(label_tensor, num_labels=2):\n",
    "    one_hot_tensor = tf.one_hot(label_tensor, num_labels)\n",
    "    return tf.reshape(one_hot_tensor, [-1, num_labels])\n",
    "\n",
    "\n",
    "\n",
    "def convert_zip_code(zip_code):\n",
    "    if zip_code == '':\n",
    "        zip_code = \"00000\"\n",
    "    else:\n",
    "        zip_code = tf.strings.regex_replace(zip_code, 'X{0,5}', \"0\")\n",
    "    return tf.strings.to_number(zip_code, out_type=tf.dtypes.int64)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    LABEL_KEY = \"consumer_disputed\"\n",
    "\n",
    "    # Feature name, feature dimensionality.\n",
    "    ONE_HOT_FEATURES = {\n",
    "    \"product\": 11,\n",
    "    \"sub_product\": 45,\n",
    "    \"company_response\": 5,\n",
    "    \"state\": 60,\n",
    "    \"issue\": 90\n",
    "    }\n",
    "\n",
    "    # Feature name, bucket count.\n",
    "    BUCKET_FEATURES = {\n",
    "    \"zip_code\": 10\n",
    "    }\n",
    "\n",
    "    # Feature name, value is unused.\n",
    "    TEXT_FEATURES = {\n",
    "    \"consumer_complaint_narrative\": None\n",
    "    }\n",
    "    outputs = {}\n",
    "    for key in ONE_HOT_FEATURES.keys():\n",
    "        dim = ONE_HOT_FEATURES[key]\n",
    "        index = tft.compute_and_apply_vocabulary(\n",
    "                fill_in_missing(inputs[key]), top_k=dim + 1)\n",
    "        outputs[transformed_name(key)] = convert_num_to_one_hot(\n",
    "                index, num_labels=dim + 1)\n",
    "\n",
    "    for key, bucket_count in BUCKET_FEATURES.items():\n",
    "        temp_feature = tft.bucketize(\n",
    "                convert_zip_code(fill_in_missing(inputs[key])),\n",
    "                bucket_count,\n",
    "                always_return_num_quantiles=False)\n",
    "        outputs[transformed_name(key)] = convert_num_to_one_hot(\n",
    "                temp_feature,\n",
    "                num_labels=bucket_count + 1)\n",
    "            \n",
    "    for key in TEXT_FEATURES.keys():\n",
    "        outputs[transformed_name(key)] = fill_in_missing(inputs[key])\n",
    "    outputs[transformed_name(LABEL_KEY)] = fill_in_missing(inputs[LABEL_KEY])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-restriction",
   "metadata": {},
   "source": [
    "#### Transform Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.types import artifact_utils\n",
    "from tfx.types import standard_artifacts\n",
    "from tfx.types import channel_utils\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = get_latest_artifacts(store, Config,PIPELINE_NAME, ['CsvExampleGen', 'ImportExampleGen'])\n",
    "\n",
    "example_gen = find_latest_artifacts_by_type(store, artifacts, standard_artifacts.Examples.TYPE_NAME)\n",
    "example_gen = channel_utils.as_channel(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = get_latest_artifacts(store, Config.PIPELINE_NAME, 'SchemaGen')\n",
    "example_schema = find_latest_artifacts_by_type(store, artifacts, standard_artifacts.Schema.TYPE_NAME)\n",
    "example_schema = channel_utils.as_channel(example_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-middle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tfx.components import Transform\n",
    "from tfx.orchestration.experimental.interactive.interactive_context \\\n",
    "        import InteractiveContext\n",
    "\n",
    "pipeline_name = Config.PIPELINE_NAME\n",
    "base_root = os.path.split(os.getcwd())[0]\n",
    "pipeline_root = os.path.join(base_root, f'temp_')\n",
    "beam_args = [\n",
    "    '--runner=DirectRunner'\n",
    "]\n",
    "\n",
    "if not os.path.exists(pipeline_root):\n",
    "    raise Exception('Run Data Ingestion Notebook before running this')\n",
    "\n",
    "context = InteractiveContext(pipeline_name = pipeline_name,\n",
    "                            pipeline_root = pipeline_root,\n",
    "                            beam_pipeline_args = beam_args)\n",
    "\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "\n",
    "transform = Transform(\n",
    "    examples=example_gen,\n",
    "    schema=example_schema,\n",
    "    module_file=os.path.join(root_dir, Config.TRANSFORM_MODULE_SCRIPT)\n",
    "\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn_graph = os.path.join(transform.outputs['transform_graph'].get()[0].uri,\n",
    "                                 'transform_fn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_graph_to_tensorboard(transform_fn_graph, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-athens",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-bronze",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "428px",
    "left": "59px",
    "top": "195px",
    "width": "258.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
