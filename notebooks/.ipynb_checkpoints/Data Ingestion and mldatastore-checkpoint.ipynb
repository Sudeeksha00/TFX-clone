{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-demographic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/dataset1\n",
    "mkdir -p ../data/dataset2\n",
    "\n",
    "python3 ../utils/download_dataset.py\n",
    "python3 ../utils/convert_to_tfrecords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-scottish",
   "metadata": {},
   "source": [
    "# Content of the Table\n",
    "\n",
    ">- [Data Ingestion](#Data-Ingestion)\n",
    ">-[What is artifact](#What-is-artifact?)\n",
    ">-[what metadata store is for?](#what-metadata-store-is-for?)\n",
    ">>- [atrifacts Tables](#atrifacts-Tables)\n",
    ">>- [Contexts Tables](#Contexts-Tables)\n",
    ">>- [Executions Tables](#Executions-Tables)\n",
    ">- [Loding dataset from tf_records](#Loding-dataset-from-tf_records)\n",
    ">-[Configuration Options](#Configuration-Options)\n",
    ">>- [splitting](#splitting)\n",
    ">>- [If data is stored in spitted manner](#If-data-is-stored-in-spitted-manner)\n",
    ">>- [Span](#Span)\n",
    ">-[Add-ons](#Add-ons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-situation",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "separated-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'absl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.orchestration.experimental.interactive.interactive_context \\\n",
    "        import InteractiveContext\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-austria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "id_ = str(datetime.datetime.now())\n",
    "pipeline_name = f'pipline_{id_}'\n",
    "base_root = os.path.split(os.getcwd())[0]\n",
    "pipeline_root = os.path.join(base_root, f'temp_')\n",
    "beam_args = [\n",
    "    '--runner=DirectRunner'\n",
    "]\n",
    "\n",
    "if not os.path.exists(pipeline_root):\n",
    "    os.makedirs(pipeline_root)\n",
    "\n",
    "\n",
    "context = InteractiveContext(pipeline_name = pipeline_name,\n",
    "                            pipeline_root = pipeline_root,\n",
    "                            beam_pipeline_args = beam_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "data_dir = os.path.join(root_dir, 'data', 'dataset1')\n",
    "\n",
    "print(*os.listdir(data_dir), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = external_input(data_dir)\n",
    "example_gen = CsvExampleGen(input = examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-cycle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_gen_prop = example_gen.outputs['examples'].get()[0]\n",
    "\n",
    "print('Artifact Location: ')\n",
    "print(f'\\t {example_gen_prop.uri}')\n",
    "print()\n",
    "\n",
    "print('Files: ')\n",
    "print('\\t train')\n",
    "print(f'\\t\\t {os.listdir(os.path.join(example_gen_prop.uri, \"train\"))}')\n",
    "print('\\t eval')\n",
    "print(f'\\t\\t {os.listdir(os.path.join(example_gen_prop.uri, \"eval\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-accident",
   "metadata": {},
   "source": [
    "### What is artifact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_names = eval(example_gen_prop.split_names)\n",
    "artifact = os.path.join(example_gen_prop.uri, split_names[0])\n",
    "files = [os.path.join(artifact, i) for i in os.listdir(artifact)]\n",
    "\n",
    "train = tf.data.TFRecordDataset(filenames = files, compression_type = 'GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train.take(1):\n",
    "    serialized_example = data.numpy()\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-interference",
   "metadata": {},
   "source": [
    "### what metadata store is for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_config = context.metadata_connection_config\n",
    "store = metadata_store.MetadataStore(connection_config)\n",
    "\n",
    "base_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_properties(input):\n",
    "    data = defaultdict(list)\n",
    "    for artifact in input:\n",
    "        properties = artifact.properties\n",
    "        custom_properties = artifact.custom_properties\n",
    "        for key, value in properties.items():\n",
    "            data['artifact id'].append(artifact.id)\n",
    "            data['type_id'].append(artifact.type_id)\n",
    "            data['name'].append(key)\n",
    "            data['is_customproperty'].append(0)\n",
    "            data['value'].append(value.string_value)\n",
    "\n",
    "            \n",
    "        for key, value in custom_properties.items():\n",
    "            data['artifact id'].append(artifact.id)\n",
    "            data['type_id'].append(artifact.type_id)\n",
    "            data['name'].append(key)\n",
    "            data['is_customproperty'].append(1)\n",
    "            data['value'].append(value.string_value)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def display_types(types):\n",
    "    table = {'id': [], 'name': []}\n",
    "    for a_type in types:\n",
    "        table['id'].append(a_type.id)\n",
    "        table['name'].append(a_type.name.split('.')[-1])\n",
    "    return pd.DataFrame(data=table)\n",
    "\n",
    "def display_artifacts(store, artifacts):\n",
    "    table = defaultdict(list)\n",
    "    for a in artifacts:\n",
    "        table['artifact id'].append(a.id)\n",
    "        artifact_type = store.get_artifact_types_by_id([a.type_id])[0]\n",
    "        table['type'].append(artifact_type.name)\n",
    "        table['uri'].append(a.uri.replace(base_dir, './'))\n",
    "        table['create_time_since_epoch'].append(a.create_time_since_epoch)\n",
    "        table['last_update_time_since_epoch'].append(a.last_update_time_since_epoch)\n",
    "    return pd.DataFrame(data=table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_context(store, artifacts):\n",
    "    table = defaultdict(list)\n",
    "    for a in artifacts:\n",
    "        table['artifact id'].append(a.id)\n",
    "        artifact_type = store.get_context_types_by_id([a.type_id])[0]\n",
    "        table['type'].append(artifact_type.name)\n",
    "        table['name'].append(a.name)\n",
    "        table['create_time_since_epoch'].append(a.create_time_since_epoch)\n",
    "        table['last_update_time_since_epoch'].append(a.last_update_time_since_epoch)\n",
    "    return pd.DataFrame(data=table)\n",
    "\n",
    "def display_executions(store, artifacts):\n",
    "    table = defaultdict(list)\n",
    "    for a in artifacts:\n",
    "        table['artifact id'].append(a.id)\n",
    "        artifact_type = store.get_execution_types_by_id([a.type_id])[0]\n",
    "        table['type'].append(artifact_type.name.split('.')[-1])\n",
    "        e_state = a.last_known_state\n",
    "        if e_state == 2:\n",
    "            table['last_known_state'].append('Running')\n",
    "        elif e_state == 3:\n",
    "            table['last_known_state'].append('Success')\n",
    "        else:\n",
    "            table['last_known_state'].append(e_state)\n",
    "        table['create_time_since_epoch'].append(a.create_time_since_epoch)\n",
    "        table['last_update_time_since_epoch'].append(a.last_update_time_since_epoch)\n",
    "    return pd.DataFrame(data=table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-singapore",
   "metadata": {},
   "source": [
    "#### atrifacts Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_artifacts(store, store.get_artifacts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_artifact_types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_properties(store.get_artifacts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-manchester",
   "metadata": {},
   "source": [
    "#### Contexts Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_context(store, store.get_contexts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_context_types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_properties(store.get_contexts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-spelling",
   "metadata": {},
   "source": [
    "#### Executions Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_executions(store, store.get_executions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_properties(store.get_executions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_execution_types())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-victor",
   "metadata": {},
   "source": [
    "## Loding dataset from tf_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import ImportExampleGen\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "data_dir = os.path.join(root_dir, 'data', 'dataset2')\n",
    "\n",
    "print(*os.listdir(data_dir), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = external_input(data_dir)\n",
    "example_gen = ImportExampleGen(input=examples)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-fountain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_executions(store, store.get_executions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-theorem",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_properties(store.get_artifacts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-browser",
   "metadata": {},
   "source": [
    "## Configuration Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-houston",
   "metadata": {},
   "source": [
    "### splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import example_gen_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-granny",
   "metadata": {},
   "source": [
    "Configuring output as train, test and eval with 6:2:2 ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir = os.path.join(os.pardir, \"data/dataset\")\n",
    "\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=6), \n",
    "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2), \n",
    "        example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=2)]\n",
    "                                                ))\n",
    "\n",
    "    examples = external_input(data_dir)\n",
    "    example_gen = CsvExampleGen(input=examples, output_config=output)\n",
    "    context.run(example_gen)\n",
    "except:\n",
    "    data_dir = os.path.join(os.pardir, \"data/dataset1\")\n",
    "\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=6), \n",
    "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2), \n",
    "        example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=2)]\n",
    "                                                ))\n",
    "\n",
    "    examples = external_input(data_dir)\n",
    "    example_gen = CsvExampleGen(input=examples, output_config=output)\n",
    "    context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tree ../temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(s):\n",
    "    if s.last_known_state == 'Running':\n",
    "        return ['background-color: red']*5\n",
    "    else:\n",
    "        return ['background-color: white']*5\n",
    "\n",
    "execution = display_executions(store, store.get_executions())\n",
    "execution.style.apply(highlight, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_id = execution['artifact id'].loc[execution.last_known_state == 'Running'].values[0]\n",
    "\n",
    "def highlight(s):\n",
    "    if s['artifact id'] == artifact_id:\n",
    "        return ['background-color: lightblue']*5\n",
    "    elif s['artifact id'] == artifact_id + 1:\n",
    "        return ['background-color: lightgreen']*5\n",
    "    else:\n",
    "        return ['background-color: white']*5\n",
    "\n",
    "execution_prop = display_properties(store.get_executions())\n",
    "execution_prop = execution_prop.loc[(execution_prop['artifact id'] == artifact_id) | (execution_prop['artifact id'] == artifact_id+1)].sort_values(by=['name','artifact id'])\n",
    "execution_prop.style.apply(highlight, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-indonesia",
   "metadata": {},
   "source": [
    "### If data is stored in spitted manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen_prop = example_gen.outputs['examples'].get()[0]\n",
    "\n",
    "shutil.copytree(example_gen_prop.uri, '../data/dataset3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import example_gen_pb2\n",
    "\n",
    "root_dir = os.path.split(os.getcwd())[0]\n",
    "data_dir = os.path.join(root_dir, 'data', 'dataset3')\n",
    "\n",
    "input = example_gen_pb2.Input(splits=[\n",
    "example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),\n",
    "example_gen_pb2.Input.Split(name='test', pattern='test/*')\n",
    "])\n",
    "\n",
    "examples = external_input(os.path.join(base_dir, data_dir))\n",
    "example_gen = ImportExampleGen(input=examples, input_config=input)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_property = display_properties(store.get_executions())\n",
    "execution_property.loc[execution_property['artifact id'] == max(execution_property['artifact id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-proxy",
   "metadata": {},
   "source": [
    "### Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/dataset4/export-0\n",
    "mkdir -p ../data/dataset4/export-1\n",
    "mkdir -p ../data/dataset4/export-2\n",
    "\n",
    "file_l_count=$(wc -l < ../data/dataset1/consumer_complaints_with_narrative.csv)\n",
    "head -n $(( file_l_count/3 )) ../data/dataset1/consumer_complaints_with_narrative.csv >> ../data/dataset4/export-0/consumer_complaints_with_narrative_$(( file_l_count/3 )).csv\n",
    "head -n $(( file_l_count/2)) ../data/dataset1/consumer_complaints_with_narrative.csv >> ../data/dataset4/export-1/consumer_complaints_with_narrative_$(( file_l_count/2 )).csv\n",
    "cp ../data/dataset1/consumer_complaints_with_narrative.csv ../data/dataset4/export-2/consumer_complaints_with_narrative_$file_l_count.csv\n",
    "\n",
    "tree ../data/dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "data_dir = os.path.join(base_dir, \"data\", \"dataset4\")\n",
    "\n",
    "\n",
    "input = example_gen_pb2.Input(splits=[\n",
    "example_gen_pb2.Input.Split(pattern='export-{SPAN}/*')\n",
    "])\n",
    "examples = external_input(data_dir)\n",
    "example_gen = CsvExampleGen(input=examples, input_config=input)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_prperties = display_properties(store.get_executions())\n",
    "temp_val = execution_prperties.loc[(execution_prperties['name'] == 'input_base') | \n",
    "                         (execution_prperties['name'] == 'span')]\n",
    "temp_val = temp_val.reset_index()\n",
    "temp_val.drop('index', axis = 1, inplace = True)\n",
    "temp_val = temp_val.sort_values(['artifact id', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_val.style.highlight_max(subset = ['value'],\n",
    "                       color = 'lightgreen', axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-boston",
   "metadata": {},
   "source": [
    "## Add-ons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-vision",
   "metadata": {},
   "source": [
    "### Ingesting Data from avro or parquest file format\n",
    "\n",
    "#### from Avro-serialized data\n",
    "\n",
    "```\n",
    "from tfx.components import FileBasedExampleGen\n",
    "from tfx.components.example_gen.custom_executors import avro_executor\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "examples = external_input(avro_dir_path)\n",
    "\n",
    "example_gen = FileBasedExampleGen(\n",
    "    input=examples,\n",
    "    executor_class=avro_executor.Executor)\n",
    "```\n",
    "\n",
    "####  from Parquet-serialized data\n",
    "\n",
    "```\n",
    "from tfx.components.example_gen.custom_executors import parquet_executor\n",
    "example_gen = FileBasedExampleGen(\n",
    "input=examples,\n",
    "executor_class=parquet_executor.Executor)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-eleven",
   "metadata": {},
   "source": [
    "### Ingesting data from Data Base\n",
    "\n",
    "#### from bigquery database\n",
    "```\n",
    "from tfx.components import BigQueryExampleGen\n",
    "query = \"\"\"\n",
    "SELECT * FROM `<project_id>.<database>.<table_name>`\n",
    "\"\"\"\n",
    "example_gen = BigQueryExampleGen(query=query)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    ">Note:\n",
    "            In TFX versions greater than 0.22.0, the BigQueryExampleGen\n",
    "            component needs to be imported from tfx.extensions.goo\n",
    "            gle_cloud_big_query :\n",
    ">```\n",
    "from tfx.extensions.google_cloud_big_query.example_gen import component as big_query_example_gen_component\n",
    "big_query_example_gen_component.BigQueryExampleGen(query=query)\n",
    ">```\n",
    "\n",
    "#### from presto database\n",
    "```\n",
    "from proto import presto_config_pb2\n",
    "from presto_component.component import PrestoExampleGen\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM `<project_id>.<database>.<table_name>`\n",
    "\"\"\"\n",
    "presto_config = presto_config_pb2.PrestoConnConfig(\n",
    "host='localhost',\n",
    "port=8080)\n",
    "example_gen = PrestoExampleGen(presto_config, query=query)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "489px",
    "left": "165px",
    "top": "246px",
    "width": "258.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
